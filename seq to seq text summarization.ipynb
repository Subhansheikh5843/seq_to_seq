{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1d412a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\Subhan\n",
      "[nltk_data]     Sheikh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\Subhan\n",
      "[nltk_data]     Sheikh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\Subhan\n",
      "[nltk_data]     Sheikh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#DataFlair Project\n",
    "#import all the required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from statistics import mode\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import LancasterStemmer\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer \n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.layers import Input,LSTM,Embedding,Dense,Concatenate,Attention\n",
    "from sklearn.model_selection import train_test_split\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0352201e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"./data/Reviews.csv\",nrows=100000)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462ac309",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(axis = 0,inplace=True)\n",
    "\n",
    "# subset=['Text']:\n",
    "# Meaning: Specifies that duplicates should be identified based only on the 'Text' column.\n",
    "df.drop_duplicates(subset=['Text'],inplace=True)\n",
    "input_data = df.loc[:,'Text']\n",
    "target_data = df.loc[:,'Summary']\n",
    "\n",
    "# Purpose: This line of code scans through the DataFrame (or Series) target and replaces every occurrence of an empty string\n",
    "# ('') with a NaN value (np.nan).\n",
    "target_data.replace('',np.nan,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae2cbfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c80dd245",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c5034b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install contractions\n",
    "# import contractions\n",
    "# Example string with contractions\n",
    "# text = \"I'm learning how to use contractions. It's a useful tool, isn't it?\"\n",
    "# # Expand contractions\n",
    "# expanded_text = contractions.fix(text)\n",
    "# print(expanded_text)\n",
    "\n",
    "# Access the internal contraction dictionary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847833d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_texts = []\n",
    "target_texts = []\n",
    "input_words = []\n",
    "target_words = []\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemm=LancasterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3a2f6d",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe55b833",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The code snippet texts = BeautifulSoup(texts, \"lxml\").text uses the BeautifulSoup \n",
    "# library to parse HTML or XML content and extract the text from it. Let's break \n",
    "# down the purpose and functionality of each part of the code:\n",
    "import contractions\n",
    "def clean(texts,src):\n",
    "    texts = BeautifulSoup(texts, \"lxml\").text\n",
    "    texts = texts.replace(\"no\", \"\")\n",
    "    words = word_tokenize(texts.lower())\n",
    "    words = list(filter(lambda w: (w.isalpha() and len(w)>=3),words))\n",
    "    words = [contractions.fix(w) for w in words]\n",
    "    if src == \"input\":\n",
    "        words = [stemm.stem(w) for w in words if w not in stop_words]\n",
    "    else:\n",
    "        words = [w for w in words if w not in stop_words]\n",
    "\n",
    "    return words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03f0236",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean(\"I'm learning how to use haven't contractions 123. It's a useful tool, isn't it?\",\"input\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7425b3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage: Typically, \"sos\" is added to the beginning of the target sequence to\n",
    "#     prompt the model to start generating the target text.\n",
    "# Usage: \"eos\" is added to the end of the target sequence to indicate where the \n",
    "#     model should stop generating text.\n",
    "for in_text,tr_text in zip(input_data,target_data):\n",
    "    \n",
    "    in_words= clean(in_text,\"input\")\n",
    "    input_texts += [\" \".join(in_words)]\n",
    "    input_words += in_words\n",
    "    tr_words = clean(\"sos\" + tr_text + \"eos\",\"target\")\n",
    "    target_texts +=[\" \".join(tr_words)]\n",
    "    target_words += tr_words\n",
    "print(\"Gfg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6866ceef",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# target_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce7b1c3b",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# input_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a72053",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_words = sorted(list(set(input_words)))\n",
    "target_words = sorted(list(set(target_words)))\n",
    "num_in_words = len(input_words)\n",
    "num_tr_words = len(target_words)\n",
    "\n",
    "# .)Now se max len of input and target text.\n",
    "max_in_len = mode([len(i) for i in input_texts])\n",
    "max_tr_len = mode([len(i) for i in target_texts])\n",
    "\n",
    "print(\"number of input words : \",num_in_words)\n",
    "print(\"number of target words : \",num_tr_words)\n",
    "print(\"maximum input length : \",max_in_len)\n",
    "print(\"maximum target length : \",max_tr_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2375b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the input and target text into 80:20 ratio or testing size of 20%.\n",
    "x_train,x_test,y_train,y_test=train_test_split(input_texts,target_texts,test_size=0.2,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20851bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train the tokenizer with all the words\n",
    "in_tokenizer = Tokenizer()\n",
    "in_tokenizer.fit_on_texts(x_train)\n",
    "tr_tokenizer = Tokenizer()\n",
    "tr_tokenizer.fit_on_texts(y_train)\n",
    " \n",
    "#convert text into sequence of integers\n",
    "#where the integer will be the index of that word\n",
    "x_train= in_tokenizer.texts_to_sequences(x_train) \n",
    "y_train= tr_tokenizer.texts_to_sequences(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d7597b",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ba7f04",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ebf6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_in_data = pad_sequences(x_train,maxlen = max_in_len,padding=\"post\")\n",
    "dec_data = pad_sequences(y_train,maxlen = max_tr_len,padding=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30468379",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder Input Data (dec_in_data):\n",
    "\n",
    "# The decoder input data (dec_in_data) represents the input sequences for the decoder,\n",
    "# excluding the last word (which is \"eos\" in this case). This is because during training,\n",
    "# the decoder is fed with its own output from the previous time step as input. So,\n",
    "# the last word (\"eos\") is removed to ensure that the model does not receive the\n",
    "# \"eos\" token as input during training.\n",
    "\n",
    "# Decoder Target Data (dec_tr_data):\n",
    "\n",
    "# The decoder target data (dec_tr_data) represents the target sequences for the decoder, \n",
    "# excluding the first word (which is \"sos\" in this case). This is because during \n",
    "# training, the model is trained to predict the next word in the sequence given\n",
    "# the previous words as input. So, the first word (\"sos\") is removed to ensure\n",
    "# that the model predicts the correct words in the sequence.\n",
    "\n",
    "\n",
    "\n",
    "# dec_data: This is the array representing the padded sequences of the decoder input data.\n",
    "#     Each row corresponds to a sequence, and each column represents a time step in \n",
    "#     the sequence.\n",
    "# [:, :-1]: This slicing operation selects all rows (:) and all columns up to the\n",
    "#     second-to-last column (:-1). It excludes the last column from each row.\n",
    "dec_in_data = dec_data[:,:-1]\n",
    "    \n",
    "#     dec_data.reshape(len(dec_data), max_tr_len, 1): This reshapes dec_data into a 3D \n",
    "#         array with dimensions (len(dec_data), max_tr_len, 1). The second dimension \n",
    "#         (max_tr_len) represents the maximum target length, and the third dimension\n",
    "#         (1) is added to make it compatible with slicing.\n",
    "# [:, 1:]: This slicing operation selects all rows and all columns starting from the second \n",
    "#     column (index 1) until the end.\n",
    "dec_tr_data = dec_data.reshape(len(dec_data),max_tr_len,1)[:,1:]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96891251",
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session() \n",
    "latent_dim = 500\n",
    " \n",
    "#create input object of total number of encoder words\n",
    "en_inputs = Input(shape=(max_in_len,)) \n",
    "\n",
    "# Purpose: This layer converts word indices into dense vectors of size latent_dim.\n",
    "#     It looks up an embedding matrix to convert each word index into a\n",
    "#     latent_dim-dimensional vector.\n",
    "# Input: The word indices from en_inputs.\n",
    "# Output: Dense vectors of size (max_in_len, latent_dim) for each input sequence.\n",
    "en_embedding = Embedding(num_in_words+1, latent_dim)(en_inputs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80bec959",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Purpose: These three stacked LSTM layers process the input sequences to capture\n",
    "#     temporal dependencies. Each LSTM layer takes the sequence output from the \n",
    "#     previous layer and produces a new sequence of the same length (max_in_len),\n",
    "#     but with updated hidden states.\n",
    "# Input: The output sequence from the previous layer (or embedding layer for the first LSTM).\n",
    "# Output: For each LSTM layer:\n",
    "# en_outputsX: The output sequences for each time step of the input \n",
    "#     (shape: (batch_size, max_in_len, latent_dim)).\n",
    "# state_hX: The hidden state at the last time step (shape: (batch_size, latent_dim)).\n",
    "# state_cX: The cell state at the last time step (shape: (batch_size, latent_dim)\n",
    "                                                \n",
    "                                                \n",
    "en_lstm1= LSTM(latent_dim, return_state=True, return_sequences=True) \n",
    "en_outputs1, state_h1, state_c1= en_lstm1(en_embedding) \n",
    " \n",
    "#LSTM2\n",
    "en_lstm2= LSTM(latent_dim, return_state=True, return_sequences=True)   \n",
    "en_outputs2, state_h2, state_c2= en_lstm2(en_outputs1) \n",
    " \n",
    "#LSTM3\n",
    "en_lstm3= LSTM(latent_dim,return_sequences=True,return_state=True)\n",
    "en_outputs3 , state_h3 , state_c3= en_lstm3(en_outputs2)\n",
    " \n",
    "# Purpose: The final hidden and cell states of the last LSTM layer in the encoder\n",
    "# are passed to the decoder as initial states to provide context for generating the\n",
    "# target sequence.\n",
    "# Output: A list containing state_h3 and state_c3.\n",
    "en_states= [state_h3, state_c3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3434419",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder. \n",
    "\n",
    "# Purpose: This layer specifies the shape of the decoder input, which is a\n",
    "#     sequence of word indices with an undefined length (None), allowing for \n",
    "#     flexible sequence lengths during training and inference.\n",
    "# Input: A batch of sequences of word indices (target sequences).\n",
    "dec_inputs = Input(shape=(None,)) \n",
    "\n",
    "# Purpose: Similar to the encoder embedding layer, it converts target word indices \n",
    "#     into dense vectors of size latent_dim.\n",
    "# Input: The word indices from dec_inputs.\n",
    "# Output: Dense vectors for the target sequences (shape: (batch_size, target_sequence_length,\n",
    "#                                                         latent_dim)).\n",
    "    \n",
    "dec_emb_layer = Embedding(num_tr_words+1, latent_dim) \n",
    "dec_embedding = dec_emb_layer(dec_inputs) \n",
    " \n",
    "    \n",
    "# Purpose: This LSTM layer processes the embedded target sequences, using the final states\n",
    "#     of the encoder LSTM as its initial states to maintain context.\n",
    "# Input: The embedded target sequences and the initial states from the encoder (en_states).\n",
    "# Output:\n",
    "# dec_outputs: The output sequences for each time step of the input (shape: (batch_size,\n",
    "#                                     target_sequence_length, latent_dim)).\n",
    "# The hidden and cell states at the last time step (not used further in this case, hence _).\n",
    "\n",
    "dec_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "dec_outputs, *_ = dec_lstm(dec_embedding,initial_state=en_states) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f053657",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Attention layer\n",
    "\n",
    "# Purpose: The attention layer allows the decoder to focus on different parts of the \n",
    "#     encoder's output sequence when generating each word in the target sequence\n",
    "#     It computes the context vector as a weighted sum of the encoder output \n",
    "#     sequences, where the weights are determined by the alignment scores between \n",
    "#     the decoder and encoder outputs.\n",
    "# Input:\n",
    "# dec_outputs: The output sequences from the decoder LSTM (query).\n",
    "# en_outputs3: The output sequences from the last encoder LSTM (value and key).\n",
    "# Output:\n",
    "# attn_out: The context vector computed by the attention mechanism \n",
    "#     (shape: (batch_size, target_sequence_length, latent_dim)).\n",
    "\n",
    "attention =Attention()\n",
    "attn_out = attention([dec_outputs,en_outputs3])\n",
    "\n",
    " \n",
    "# Purpose: This layer concatenates the attention output (attn_out) with the decoder \n",
    "#     outputs (dec_outputs) along the last axis to combine context information from \n",
    "#     the encoder with the current state of the decoder.\n",
    "# Input:\n",
    "# dec_outputs: The output sequences from the decoder LSTM.\n",
    "# attn_out: The context vector from the attention layer.\n",
    "# Output: A combined representation of the decoder output and attention context \n",
    "#     (shape: (batch_size, target_sequence_length, 2 * latent_dim)).\n",
    "merge=Concatenate(axis=-1, name='concat_layer1')([dec_outputs,attn_out])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bba752c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Purpose: This layer produces the final output probabilities for each word\n",
    "#     in the target vocabulary. It uses the softmax activation function\n",
    "#     to generate a probability distribution over all possible words in the vocabulary.\n",
    "# Input: The concatenated output from the merge layer.\n",
    "# Output: Probability distributions over the target vocabulary for each time step \n",
    "#     of the target sequence (shape: (batch_size, target_sequence_length, num_tr_words + 1))\n",
    "    \n",
    "#Dense layer (output layer)\n",
    "dec_dense = Dense(num_tr_words+1, activation='softmax') \n",
    "dec_outputs = dec_dense(merge) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bed527c",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# #Model class and model summary for text Summarizer\n",
    "\n",
    "# Purpose: This defines the full sequence-to-sequence model with attention.\n",
    "#     It takes the encoder and decoder inputs and produces the final target sequence \n",
    "#     probabilities.\n",
    "# Input:\n",
    "# en_inputs: The input sequences for the encoder.\n",
    "# dec_inputs: The input sequences for the decoder.\n",
    "# Output: The probability distributions over the target vocabulary for each time step\n",
    "#     of the target sequence.\n",
    "    \n",
    "model = Model([en_inputs, dec_inputs], dec_outputs) \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50f97e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile( \n",
    "    optimizer=\"rmsprop\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"] ) \n",
    "model.fit( \n",
    "    [en_in_data, dec_in_data],\n",
    "    dec_tr_data, \n",
    "    batch_size=512, \n",
    "    epochs=10, \n",
    "    validation_split=0.1,\n",
    "    )\n",
    " \n",
    "#Save model\n",
    "model.save(\"s2s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed870bf4",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab7edb6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f05f27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305fe81f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387f1e30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e9e351",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef39f182",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86794449",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6d0063",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5937b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a814fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb60c17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3bca765",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce1ea92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41df4e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb27a06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51b7a93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce0f424",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d730c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d6c38d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6330a90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02dbdbe5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede77eae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50579ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277bd28c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5ba463",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97dee92",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
